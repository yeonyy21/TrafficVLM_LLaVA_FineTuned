import os
import base64
import openai
import chromadb
import random

# --------------------------------------------------------------------------------------
# ì„¹ì…˜ 1: ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# --------------------------------------------------------------------------------------
OPENAI_API_KEY='your_key'
GPT_VISION_MODEL = "gpt-4o"
GPT_REASONING_MODEL = "gpt-4o"
EMBEDDING_MODEL = "text-embedding-3-small"

KNOWLEDGE_BASE = [
    { "id": "law_50_3", "content": "ë„ë¡œêµí†µë²• ì œ50ì¡° ì œ3í•­: ì´ë¥œìë™ì°¨ì™€ ì›ë™ê¸°ì¥ì¹˜ìì „ê±°ì˜ ìš´ì „ìëŠ” ì¸ëª…ë³´í˜¸ ì¥êµ¬(ì•ˆì „ëª¨)ë¥¼ ì°©ìš©í•˜ê³  ìš´í–‰í•˜ì—¬ì•¼ í•˜ë©°, ë™ìŠ¹ìì—ê²Œë„ ì´ë¥¼ ì°©ìš©í•˜ë„ë¡ í•˜ì—¬ì•¼ í•œë‹¤. ìœ„ë°˜ ì‹œ ë²”ì¹™ê¸ˆì€ 2ë§Œì›ì´ë‹¤." },
    { "id": "law_5", "content": "ë„ë¡œêµí†µë²• ì œ5ì¡°: ëª¨ë“  ì°¨ì˜ ìš´ì „ìëŠ” ì‹ í˜¸ê¸°ê°€ í‘œì‹œí•˜ëŠ” ì‹ í˜¸ ë˜ëŠ” êµí†µì •ë¦¬ë¥¼ í•˜ëŠ” ê²½ì°°ê³µë¬´ì›ë“±ì˜ ì‹ í˜¸ë¥¼ ë”°ë¼ì•¼ í•œë‹¤. ì‹ í˜¸ìœ„ë°˜ ì‹œ ë²”ì¹™ê¸ˆì€ 6ë§Œì›, ë²Œì ì€ 15ì ì´ë‹¤." },
    { "id": "law_13_1", "content": "ë„ë¡œêµí†µë²• ì œ13ì¡° ì œ1í•­: ì°¨ë§ˆì˜ ìš´ì „ìëŠ” ë³´ë„ì™€ ì°¨ë„ê°€ êµ¬ë¶„ëœ ë„ë¡œì—ì„œëŠ” ì°¨ë„ë¡œ í†µí–‰í•˜ì—¬ì•¼ í•œë‹¤. ë³´ë„ í†µí–‰ìœ„ë°˜ ì‹œ ë²”ì¹™ê¸ˆì€ 4ë§Œì›, ë²Œì ì€ 10ì ì´ë‹¤." }
]

try:
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    client.models.list()
except openai.AuthenticationError:
    print(f"[ì˜¤ë¥˜] OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    exit()
except Exception as e:
    print(f"[ì˜¤ë¥˜] OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
    exit()

# --------------------------------------------------------------------------------------
# ì„¹ì…˜ 2: ê¸°ì¡´ ë¡œì»¬ ëª¨ë¸ ì—°ë™
# --------------------------------------------------------------------------------------
try:
    import infer_final
    print("âœ… [ì—°ë™] ë¡œì»¬ í—¬ë©§ íƒì§€ ëª¨ë¸('infer_final.py')ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    infer_final._initialize_model_and_processor_for_inference()
    print("âœ… [ì—°ë™] ë¡œì»¬ VLMì´ ì¶”ë¡  ì¤€ë¹„ë¥¼ ë§ˆì³¤ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"[ì˜¤ë¥˜] ë¡œì»¬ ëª¨ë¸ ì—°ë™ ë˜ëŠ” ì´ˆê¸°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
    exit()

# --------------------------------------------------------------------------------------
# ì„¹ì…˜ 3: íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ì •ì˜ (GPT-4o Vision ì¬ë„ì…)
# --------------------------------------------------------------------------------------

def setup_chroma_db(knowledge_base: list, chroma_client: chromadb.Client, openai_client: openai.OpenAI, collection_name="traffic_law"):
    collection = chroma_client.get_or_create_collection(name=collection_name)
    if collection.count() == 0:
        documents = [item['content'] for item in knowledge_base]
        ids = [item['id'] for item in knowledge_base]
        response = openai_client.embeddings.create(input=documents, model=EMBEDDING_MODEL)
        embeddings = [embedding.embedding for embedding in response.data]
        collection.add(embeddings=embeddings, documents=documents, ids=ids)
    return collection

def get_rich_description_from_gpt4v(image_path: str) -> str:
    """GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸í•œ ìƒí™© ë¬˜ì‚¬ë¥¼ ì–»ìŠµë‹ˆë‹¤."""
    print(f"ğŸ¤– [GPT-4o Vision] ì´ë¯¸ì§€ ì‹¬ì¸µ ë¶„ì„ ìš”ì²­: '{image_path}'")
    try:
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')

        specific_prompt = "ì´ê²ƒì€ ì•ˆì „ëª¨ ë¯¸ì°©ìš©ì´ ì˜ì‹¬ë˜ëŠ” ì¸ë¬¼ì˜ í¬ë¡­ ì´ë¯¸ì§€ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ì† ì¸ë¬¼ì´ ë¨¸ë¦¬ì— ë¬´ì—‡ì„ ì“°ê³  ìˆëŠ”ì§€, ë˜ëŠ” ì•„ë¬´ê²ƒë„ ì“°ì§€ ì•Šì•˜ëŠ”ì§€ ì‚¬ì‹¤ ê·¸ëŒ€ë¡œ í•œêµ­ì–´ë¡œ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”."
        response = client.chat.completions.create(
            model=GPT_VISION_MODEL,
            messages=[{"role": "user", "content": [{"type": "text", "text": specific_prompt}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}]}]
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"[ì˜¤ë¥˜] GPT-4o Vision API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def query_rag_pipeline(query_text: str, collection: chromadb.Collection, openai_client: openai.OpenAI) -> str:
    """RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"ğŸ” [RAG] ê´€ë ¨ ë²•ê·œ ê²€ìƒ‰ ì‹¤í–‰. Query: \"{query_text[:30]}...\"")
    try:
        query_embedding = openai_client.embeddings.create(input=[query_text], model=EMBEDDING_MODEL).data[0].embedding
        retrieved_results = collection.query(query_embeddings=[query_embedding], n_results=1)
        retrieved_context = retrieved_results['documents'][0][0]

        system_prompt = "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ìƒí™© ë¬˜ì‚¬]ì™€ ê´€ë ¨ [ë²•ê·œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì–´ë–¤ ë²•ì„ ìœ„ë°˜í–ˆëŠ”ì§€ ëª…ì‹œí•˜ê³  ì²˜ë¶„ ì‚¬í•­ì„ ìš”ì•½í•˜ì—¬ ë¦¬í¬íŠ¸ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        human_prompt = f"[ìƒí™© ë¬˜ì‚¬]\n{query_text}\n\n[ê´€ë ¨ ë²•ê·œ]\n{retrieved_context}"
        response = client.chat.completions.create(
            model=GPT_REASONING_MODEL,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": human_prompt}],
            temperature=0.2,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"[ì˜¤ë¥˜] RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ë²•ë¥  ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# --------------------------------------------------------------------------------------
# ì„¹ì…˜ 4: ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# --------------------------------------------------------------------------------------

if __name__ == "__main__":
    crops_dir = "data/crops"
    NUM_SAMPLES_TO_TEST = 13

    all_images = [f for f in os.listdir(crops_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))] if os.path.isdir(crops_dir) else []
    if not all_images:
        print(f"[ì‹¤í–‰ ì˜¤ë¥˜] '{crops_dir}' í´ë”ì— ë¶„ì„í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    images_to_process = random.sample(all_images, min(len(all_images), NUM_SAMPLES_TO_TEST))

    print(f"\nğŸš€ VLM-RAG êµí†µ ë²•ê·œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤. ğŸš€\n{len(images_to_process)}ê°œì˜ ìƒ˜í”Œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n" + "="*60)

    chroma_client = chromadb.Client()
    law_collection = setup_chroma_db(KNOWLEDGE_BASE, chroma_client, client)

    for i, filename in enumerate(images_to_process):
        image_to_analyze = os.path.join(crops_dir, filename)

        print(f"\n[{i+1}/{len(images_to_process)}] >> ë¶„ì„ ì‹œì‘: {image_to_analyze}")
        print("-" * 60)

        print("--- 1ë‹¨ê³„: ë¡œì»¬ ì „ë¬¸ ëª¨ë¸ë¡œ ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹ ---")
        initial_check_result = infer_final.predict_helmet_with_llava_next(image_to_analyze)
        print(f"ğŸ‘¤ [ë¡œì»¬ VLM ê²°ê³¼] í—¬ë©§ ì°©ìš© ì—¬ë¶€: '{initial_check_result}'")

        if "ì•„ë‹ˆìš”" in initial_check_result:
            print("\n[ì•Œë¦¼] í—¬ë©§ ë¯¸ì°©ìš© ê°ì§€. ì‹¬ì¸µ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n")

            # GPT-4o Visionì„ ì´ìš©í•œ ì‹¬ì¸µ ë¶„ì„ í˜¸ì¶œ
            print("--- 2ë‹¨ê³„: ë²”ìš© VLM(GPT-4o)ì„ ì´ìš©í•œ ì‹¬ì¸µ ìƒí™© ë¶„ì„ ---")
            rich_description = get_rich_description_from_gpt4v(image_to_analyze)
            print(f"ğŸ’¬ [GPT-4o VLM ë¬˜ì‚¬]\n{rich_description}\n")

            if "ì˜¤ë¥˜" not in rich_description:
                print("--- 3ë‹¨ê³„: RAG + LLM ê¸°ë°˜ ë²•ë¥  ë¶„ì„ ---")
                final_report = query_rag_pipeline(rich_description, law_collection, client)
                print("\n" + "="*50 + "\nâœ… ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸\n" + "="*50)
                print(final_report)
        else:
            print("\n[ì•Œë¦¼] í—¬ë©§ ì°©ìš©ìœ¼ë¡œ íŒë‹¨. ì‹¬ì¸µ ë¶„ì„ì„ ì§„í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        print("=" * 60)

    print("\nâœ… ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
